{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"mount_file_id":"1Zo9lsnzNoBNQwYLsXlXGO4dbs7genhdc","authorship_tag":"ABX9TyPvAeKdaz0ImeAToy7GC4I4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"iyqdQQsnG4dH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bYFFAZ3qW-_"},"outputs":[],"source":["# Let`s import all packages that we may need:\n","\n","import sys \n","import numpy as np # linear algebra\n","from scipy.stats import randint\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n","import matplotlib.pyplot as plt # this is used for the plot the graph \n","import seaborn as sns # used for plot interactive graph. \n","from sklearn.model_selection import train_test_split # to split the data into two parts\n","from sklearn.preprocessing import StandardScaler # for normalization\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.pipeline import Pipeline # pipeline making\n","from sklearn.model_selection import cross_val_score\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn import metrics # for the check the error and accuracy of the model\n","from sklearn.metrics import mean_squared_error,r2_score\n","from math import sqrt\n","from pandas import concat\n","from sklearn.preprocessing import LabelEncoder\n","\n","## for Deep-learing:\n","import keras\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from keras.utils import to_categorical\n","from keras.optimizers import SGD \n","from keras.callbacks import EarlyStopping\n","from keras.utils import np_utils\n","import itertools\n","from keras.layers import LSTM\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers import Dropout"]},{"cell_type":"code","source":["# Import data, convert string dates to 'datetime64' and set the date column as index:\n","\n","df = pd.read_csv('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/fase2/Procesados/TISIMData_Overtaking_fase2_tarea_velocidad.csv',\n","                 sep=',', \n","                 parse_dates={'dt' : ['Elapsed time']}, infer_datetime_format=True, \n","                 low_memory=False, na_values=['nan','?'], index_col='dt')\n","\n","df2 = pd.read_csv('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/fase2/Procesados/TISIMData_Turnings_fase2_tarea_velocidad.csv',\n","                 sep=',', \n","                 parse_dates={'dt' : ['Elapsed time']}, infer_datetime_format=True, \n","                 low_memory=False, na_values=['nan','?'], index_col='dt')\n","\n","#  Review the general info on data, paying attention to missing values and dtypes\n","df.info()"],"metadata":{"id":"t27DrqjFscCw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2.info()"],"metadata":{"id":"0bL3P8MGMXJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_order = [\"Clasification\", \"Gas pedal\", \"speed\", \"Brake pedal\", \"Clutch pedal\", \"Long Dist\", \"Lat Pos\",\"Steering wheel angle\",\"Throttle input\", \"Brake pedal force\", \"Left turn\", \"Right turn\", \"Gear\", \"RPM\", \"Hand wheel torque\",\"New_Target\"]\n","df = df.reindex(columns=new_order)\n","df2 = df2.reindex(columns=new_order)"],"metadata":{"id":"por_qmBq-xUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"WHtLxJWpJVks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2.head()"],"metadata":{"id":"_tKRJEn8MclZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"QJ0Q82SA2B6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape\n"],"metadata":{"id":"-qlJW5ecHtmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2.shape"],"metadata":{"id":"hMfKQ9pTMkm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"1JwUiBmZJnVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2.columns"],"metadata":{"id":"rXqaNAx3MqoQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"FS0a9nXHJ5us"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"5Pqxt9KsMs1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the time series\n","plt.style.use('fivethirtyeight')\n","df.plot(subplots=True,\n","        layout=(6, 3),\n","        figsize=(22,22),\n","        fontsize=10, \n","        linewidth=2,\n","        sharex=False,\n","        title='Visualization of the original Time Series')\n","plt.savefig('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking_time_series.png')\n","plt.show()"],"metadata":{"id":"c7xuvw-dKBCm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's also draw a heatmap visualization of the correlation matrix\n","corr_matrix = df.corr(method='spearman')\n","f, ax = plt.subplots(figsize=(18,8))\n","sns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidth=0.4,\n","            annot_kws={\"size\": 10}, cmap='coolwarm', ax=ax)\n","plt.xticks(fontsize=10)\n","plt.yticks(fontsize=10)\n","plt.savefig('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking_correlation_matrix.png')\n","plt.show()"],"metadata":{"id":"isFbPrtvNemi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load dataset\n","values = df.values\n","values2 = df2.values\n","# normalize features\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled = scaler.fit_transform(values)\n","scaled2 = scaler.fit_transform(values2)\n","# specify the number of lag hours\n","#Posibles valores 4 -> 15\n","n_timesteps = 4\n","n_features = 15\n","print(df.shape)\n","print(df.head())\n","# split into train and test sets\n","train =  df.values\n","test = df2.values\n","# split into input and outputs\n","n_obs = n_timesteps * n_features\n","train_X, train_y = train[:, :n_obs], train[:, -1]\n","test_X, test_y = test[:, :n_obs], test[:, -1]\n","print(train_X.shape, len(train_X), train_y.shape)\n","print(train_X.shape[0])\n","# reshape input to be 3D [samples, timesteps, features]\n","train_X = train_X.reshape((train_X.shape[0], n_timesteps, -1))\n","test_X = test_X.reshape((test_X.shape[0], n_timesteps, -1))\n","print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"],"metadata":{"id":"qIB5BrLFf-lZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelBinarizer\n","\n","lb = LabelBinarizer()\n","train_y_transformed = lb.fit_transform(train_y)\n","test_y_transformed = lb.transform(test_y)\n","\n","# Imprimir las primeras 5 etiquetas originales y sus correspondientes matrices binarias\n","print(train_y[:6])\n","print(train_y_transformed[:6])\n","\n","# COMPROBAR TIPO DE LOS DATOS\n","print(type(train_X))\n","print(type(train_y_transformed))\n","\n","# COMPROBAR DIMENSIONES DE LOS DATOS\n","print(train_X.shape)\n","print(train_y.shape)\n","print(train_y_transformed.shape)\n","print(test_X.shape)\n","print(test_y.shape)\n","print(test_y_transformed.shape)"],"metadata":{"id":"id5qv9eak87N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OBTENER DIMENSION DE LA ENTRADA Y NÚMERO DE SALIDAS\n","input_shape = (train_X.shape [1] ,)\n","num_clases = test_y_transformed.shape [1]\n","# NUMERO DE PATRONES POR CLASES\n","print('Numero de clases:\\n', num_clases)"],"metadata":{"id":"rrPwsIHLIhGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ESTRATIFICACION DE LOS DATOS\n","from sklearn.model_selection import train_test_split\n","train2_X, Validation_X, train_y_transformed2, Validation_y = train_test_split(train_X,train_y_transformed,stratify=train_y_transformed,test_size=0.2)"],"metadata":{"id":"CcJWKRtySiVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#OPCIÓN 2 Early Stopping\n","from keras.callbacks import ModelCheckpoint\n","checkpoint = ModelCheckpoint(\"content/sample_data/best\",monitor='val_loss',verbose=1,save_best_only=True)\n","# FIJAR SEMILLA ALEATORIA\n","from numpy.random import seed\n","from keras.utils import set_random_seed\n","semilla = 10\n","seed(semilla)\n","set_random_seed(semilla)"],"metadata":{"id":"OKanx5LOLHia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ASIGNAR PESOS Y DEFINIR UN DICCIOANRIO\n","peso_clase0=1.\n","peso_clase1=2. #Clase1: 2 instancias de clase 1\n","peso_clase2=4. #Clase2: 4 instancias de clase 2\n","peso_clase3=6. #Clase3: 6 instancias de clase 3\n","peso_clase4=12. #Clase4: 12 instancias de clase 4\n","peso_clase5=8. #Clase5: 8 instancias de clase 5\n","peso_clase6=10. #Clase6: 10 instancias de clase 6\n","class_weight = {0: peso_clase0, 1: peso_clase1,2: peso_clase2,3: peso_clase3,4: peso_clase4,5: peso_clase5,6: peso_clase6}\n","#ASIGNAR PESOS EN BASE A LOS DATOS\n","from sklearn.utils import class_weight\n","#Es necesario que y_train sea un array\n","pesos = class_weight.compute_class_weight('balanced', classes=np.unique(train_y),y=train_y)\n","print(pesos)\n","#CREACIÓN DEL DICCIONARIO (Una opción)\n","class_weight = {0: int(pesos[0]+1),1: int(pesos[1]*4),2: int(pesos[2]*3+1),3: int(pesos[3]*6),4: int(pesos[4]*5+1),5: int(pesos[5]*2),6: int(pesos[6]*5)}\n","print(class_weight)"],"metadata":{"id":"emhrjNnvOs_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#128 neuronas ocultas primera capa\n","num_hidden_neurons = 128\n","#Epocas = 100\n","epochs = 120\n","#Dropout 50% = 0.5\n","dropout = 0.2 \n","#batch_size = 64\n","batch_size = 72\n","#Funcion de activacion sigmoid\n","activation = 'relu' \n","#1 neurona en la capa de salida\n","#Mean absolute error (MAE)\n","#Optimizador Adam (versión eficiente del descenso de gradiente)\n","\n","model = Sequential()\n","model.add(LSTM(num_hidden_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n","model.add(Dropout(dropout))\n","#model.add(keras.layers.Reshape((1, 128))) # Agrega esta capa para cambiar la forma de entrada a (batch_size, timesteps, features)\n","model.add(Dense(num_hidden_neurons, activation = activation))\n","model.add(Dense(num_clases,activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['mse','accuracy'])\n","\n","cadena = str(epochs) + str(num_hidden_neurons) + activation + str(n_timesteps)\n","\n","# fit network\n","history = model.fit(train2_X, train_y_transformed2, epochs=epochs, batch_size=batch_size, callbacks=[checkpoint], validation_data=(Validation_X, Validation_y), verbose=2, shuffle=False, validation_freq=1, class_weight=class_weight)\n","ephocs_stop=np.where(history.history['val_loss'] == np.min(history.history['val_loss']))\n","final_epoch=ephocs_stop[0][0]\n","print(final_epoch)\n","print(history.history['loss'][final_epoch-1])\n","print(history.history['val_loss'][final_epoch-1])\n","print(history.history['accuracy'][final_epoch-1])\n","print(history.history['val_accuracy'][final_epoch-1])\n","\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.savefig('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking' + cadena + '_accuracy.png')\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper right')\n","plt.savefig('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking' + cadena + '_loss.png')\n","plt.show()\n","#GUARDAR EN FICHERO LOSS Y ACCURACY EN CADA ÉPOCA\n","np.savetxt('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking' + cadena + '_historicoTrainLoss.txt',history.history['loss'])\n","np.savetxt('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking' + cadena + '_historicoValLoss.txt',history.history['val_loss'])\n","np.savetxt('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking' + cadena + '_historicoTrainAcc.txt',history.history['accuracy'])\n","np.savetxt('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking' + cadena + '_historicoValAcc.txt',history.history['val_accuracy'])"],"metadata":{"id":"DY68RxBHPxrM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EVALUAR MODELO DEFINITIVO\n","train_results = model.evaluate(train_X, train_y_transformed, verbose=1)\n","test_results = model.evaluate(test_X, test_y_transformed, verbose=1)\n","#EL INDICE 0 ES EL LOSS. EL RESTO, LAS MÉTRICAS QUE SE HAN ESPECIFICADO AL COMPILAR EL MODELO.\n","#EN ESTE CASO 'accuracy':1,'mse':2\n","print(f'Train results - Loss: {train_results[0]} - Accuracy: {train_results[1]} - MSE: {train_results[2]}')\n","print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]} - MSE: {test_results[2]}')\n","\n","# PREDICCIONES DE LAS CLASES\n","# PREDICCIONES EN BRUTO\n","raw_testPred = model.predict(test_X)\n","print(raw_testPred[:5])\n","# PREDICCIONES DE LA CLASE\n","testPred = np.argmax(raw_testPred, axis=1)\n","class_labels = ['Cont', 'ac recto', 'red recto', 'ac izq', 'red izq', 'ac dch', 'red dch']\n","#MATRIZ DE CONFUSIÓN Y OTRAS MÉTRICAS\n","from sklearn.metrics import confusion_matrix, classification_report\n","cm=confusion_matrix(test_y, testPred)\n","print('Matriz de confusión')\n","print(cm)\n","# crear mapa de calor dibujar mapa de calor\n","dataframe = pd.DataFrame(cm)\n","sns.heatmap(cm, xticklabels = class_labels, yticklabels = class_labels, annot = True, linewidths = 0.1, fmt='d', cmap = 'YlGnBu')\n","#sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\"), plt.tight_layout()\n","plt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\n","plt.savefig('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/results/Fase2/Overtaking' + cadena + '_confusionMatrix.png')\n","plt.show()\n","print('Classification Report')\n","print(classification_report(test_y, testPred))"],"metadata":{"id":"T9oGRi8rVlyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crea un nuevo dataframe\n","output_data = pd.DataFrame()\n","\n","# Agrega las columnas del dataframe\n","output_data['Predictions'] = testPred\n","\n","# Guarda el nuevo dataframe en un archivo\n","output_data.to_csv('/content/drive/MyDrive/UNIVERSIDAD/CUARTO/TFG/final/Datasets/Overtaking' + cadena + '.csv', index=False)"],"metadata":{"id":"NBUZFRLKyHFF"},"execution_count":null,"outputs":[]}]}